{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6fa6e7",
   "metadata": {},
   "source": [
    "# 一、粗糙版GRPO伪代码\n",
    "\n",
    "存在的问题：\n",
    "- 1. KL散度非标准实现，基于a泰勒展开式的二阶估计\n",
    "- 2. 未对优势值展开具体讲解，即过程监督和结果监督\n",
    "- 3. 未对价值模型是如何被省略做具体讲解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805369a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    reference_model = copy.deepcopy(policy_model)\n",
    "    \n",
    "    for _ in range(setps_per_iteration):\n",
    "        # 便利提示数据的每个批次\n",
    "        # 每个批次的prompt重复num_generations次，生成num_generations次，生成num_generations个响应\n",
    "        batch_prompt = random.sample(prompts, batch_size)\n",
    "        batch_prompt = batch_prompt.repeat_interleave(num_generations)\n",
    "        # 使用当前policy模型根据批次提示生成响应\n",
    "        batch_response = actor_model.gengerate(batch_prompt)\n",
    "        # 将批次提示和生成的响应拼接在一起\n",
    "        batch_data = concat(batch_prompt, batch_response)\n",
    "        \n",
    "        # 奖励模型有多个，每个奖励模型对数据进行打分，得到多个奖励，最后相加得到最终奖励\n",
    "        batch_rewards = reward_model(batch_data)\n",
    "        \n",
    "        # 前向传播当前policy模型，得到所有可能动作/词元的完整概率分布，实际生成序列的动作、词元的具体概率\n",
    "        old_actor_all_probs, old_actor_probs = actor_model.forward(batch_data)\n",
    "        # 前向传播参考模型，得到所有可能动作、词元的g完整概率分布、实际生成序列的动作、词元的具体概率\n",
    "        ref_all_probs, ref_probs = ref_model.forward(batch_data)\n",
    "        \n",
    "        # 优势计算有两种方式：过程监督和结果监督\n",
    "        advantages = compute_advantages(batch_rewards)\n",
    "        \n",
    "        # 重要性采样\n",
    "        for _ in range(mu):\n",
    "            # 前向传播当前策略模型，得到所有可能动作/词元的额完整概率分布、实际生成序列的动作、词元的具体概率和所有值\n",
    "            # actor_all_probs: actor模型对所有可能动作/词元的完整概率分布（logits或log probabilities）\n",
    "            # actor_probs: actor模型对实际生成序列的动作/词元的具体概率(即生成轨迹的概率)\n",
    "            new_actor_all_probs, new_actor_probs = actor_model.forward(batch_data)\n",
    "            \n",
    "            # 计算新、旧policy概率比\n",
    "            ratio = new_actor_probs / old_actor_probs\n",
    "            \n",
    "            # 计算优势损失损失\n",
    "            loss_adv = torch.mean(-advantages * ratio)\n",
    "            \n",
    "            # 计算当前策略模型和参考模型之间的KL散度损失（KL惩罚项， penalty）\n",
    "            # per_torken_kl = torch.exp(ref_all_probs - new_actor_all_probs) - (ref_all_probs - new_actor_all_probs) - 1\n",
    "            loss_kl = compute_KL(new_actor_all_probs, ref_all_probs)\n",
    "            \n",
    "            # 计算总损失，由actor损失和critic损失加权求和得到\n",
    "            loss = loss_adv + self.beta * loss_kl\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3db03",
   "metadata": {},
   "source": [
    "# 二、详细版GRPO代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# 超参数\n",
    "batch_size = 32          # 批量大小\n",
    "num_generations = 8      # 每条提示生成 n 条序列\n",
    "max_len = 128            # 最大生成长度\n",
    "epsilon = 0.2            # PPO 裁剪范围\n",
    "beta = 0.01              # KL 惩罚系数\n",
    "gamma = 0.99             # 折扣因子（过程监督）\n",
    "mu = 10                  # 每个批次的优化步数\n",
    "num_iterations = 100     # 总迭代次数\n",
    "PAD_TOKEN_ID = 0         # Padding token ID\n",
    "SEP_TOKEN_ID = 50256     # 分隔符 token ID（例如，GPT-2）\n",
    "\n",
    "# 假设模型和数据(需要训练的大模型：deepseek)\n",
    "policy_model = nn.Module()    # 策略模型（例如，Transformer）\n",
    "ref_model = copy.deepcopy(policy_model)  # 参考模型（SFT 模型）\n",
    "ref_model.eval()\n",
    "\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "reward_models = [nn.Module()]  # 奖励模型列表（支持多个），这里简化为１个\n",
    "weights = [1.0]               # 每个奖励模型的权重，这里简化为１个\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)\n",
    "prompts = [...]               # 提示数据集（列表或张量）\n",
    "\n",
    "def gather_log_probs(logits, actions):\n",
    "    \"\"\"从 logits 中提取实际动作的对数概率\"\"\"\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)  # [batch_size * n, seq_len, vocab_size]\n",
    "    return torch.gather(log_probs, dim=-1, index=actions.unsqueeze(-1)).squeeze(-1)  # [batch_size * n, seq_len]\n",
    "\n",
    "def compute_KL(new_logits, ref_logits, response_mask):\n",
    "    \"\"\"\n",
    "    计算标准KL散度（方式１：只计算被采样到的动作的KL散度）\n",
    "    输入：\n",
    "        new_logits: 新策略 logits [batch_size * n, seq_len]\n",
    "        ref_logits: 参考策略 logits [batch_size * n, seq_len]\n",
    "    --------------------------------------------------------------------\n",
    "    计算标准 KL 散度（方式 2：整个词汇表概率分布）\n",
    "    输入：\n",
    "        new_logits: 新策略 logits [batch_size * n, seq_len, vocab_size]\n",
    "        ref_logits: 参考策略 logits [batch_size * n, seq_len, vocab_size]\n",
    "        response_mask: 响应掩码 [batch_size * n, seq_len]\n",
    "    输出：\n",
    "        kl: 平均 KL 散度（标量）\n",
    "    \"\"\"\n",
    "    new_probs = torch.softmax(new_logits, dim=-1)  # [batch_size * n, seq_len, vocab_size]\n",
    "    ref_probs = torch.softmax(ref_logits, dim=-1)  # [batch_size * n, seq_len, vocab_size]\n",
    "    kl = torch.sum(new_probs * (torch.log(new_probs + 1e-10) - torch.log(ref_probs + 1e-10)), dim=-1)  # [batch_size * n, seq_len]\n",
    "    return (kl * response_mask).sum() / response_mask.sum()  # 忽略 padding\n",
    "\n",
    "def compute_process_advantages(batch_rewards, batch_size, num_generations, seq_len, gamma=0.99):\n",
    "    \"\"\"\n",
    "    过程监督优势计算\n",
    "    - 逐 token 奖励，基于后续累积奖励的归一化\n",
    "    - 公式：A_{i,t} = (R_{i,t} - mean({R_{j,t}})) / std({R_{j,t}})\n",
    "    - R_{i,t} = r_{i,t} + γ r_{i,t+1} + γ^2 r_{i,t+2} + ...\n",
    "    输入：\n",
    "        batch_rewards: 逐 token 奖励 [batch_size * n, seq_len]\n",
    "        batch_size: 批量大小\n",
    "        num_generations: 每条提示生成 n 条序列\n",
    "        seq_len: 序列长度\n",
    "        gamma: 折扣因子\n",
    "    输出：\n",
    "        advantages: 逐 token 优势 [batch_size * n, seq_len]\n",
    "    \"\"\"\n",
    "    rewards = batch_rewards.view(batch_size, num_generations, seq_len)  # [batch_size, n, seq_len]\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    # 动态规划计算累积奖励\n",
    "    future_rewards = torch.zeros(batch_size, num_generations).to(rewards.device)\n",
    "    for t in range(seq_len - 1, -1, -1):\n",
    "        future_rewards = rewards[:, :, t] + gamma * future_rewards  # R_{i,t} = r_{i,t} + γ R_{i,t+1}\n",
    "        mean_rewards = future_rewards.mean(dim=1, keepdim=True)  # 均值：1/n ∑ R_{j,t}\n",
    "        std_rewards = future_rewards.std(dim=1, keepdim=True) + 1e-8  # 标准差：std({R_{j,t}})\n",
    "        advantages[:, :, t] = (future_rewards - mean_rewards) / std_rewards  # 归一化\n",
    "    return advantages.view(batch_size * num_generations, seq_len)\n",
    "\n",
    "def compute_outcome_advantages(batch_rewards, batch_size, num_generations):\n",
    "    \"\"\"\n",
    "    结果监督优势计算\n",
    "    - 序列级奖励，基于群体归一化\n",
    "    - 公式：A_i = (r_i - mean({r_j})) / std({r_j})\n",
    "    输入：\n",
    "        batch_rewards: 序列级奖励 [batch_size * n]\n",
    "        batch_size: 批量大小\n",
    "        num_generations: 每条提示生成 n 条序列\n",
    "    输出：\n",
    "        advantages: 序列级优势，扩展到逐 token [batch_size * n, seq_len]\n",
    "        \n",
    "    每个token使用序列的归一化优势。就是同一个生成序列的所有token是同一个优势值\n",
    "    \"\"\"\n",
    "    rewards = batch_rewards.view(batch_size, num_generations)  # [batch_size, n]\n",
    "    mean_rewards = rewards.mean(dim=1, keepdim=True)  # 均值：1/n ∑ r_j\n",
    "    std_rewards = rewards.std(dim=1, keepdim=True) + 1e-8  # 标准差：std({r_j})\n",
    "    advantages = (rewards - mean_rewards) / std_rewards  # 归一化\n",
    "    return advantages.view(batch_size * num_generations, 1).expand(-1, max_len)  # 扩展到 [batch_size * n, seq_len]\n",
    "\n",
    "# GRPO 训练循环\n",
    "for iteration in range(num_iterations):\n",
    "    # 1. 数据准备\n",
    "    batch_prompt = random.sample(prompts, batch_size)  # 随机采样提示\n",
    "    batch_prompt = torch.tensor(batch_prompt).repeat_interleave(num_generations, dim=0).to(device)  # [batch_size * n, len_prompt]\n",
    "    \n",
    "    # 说明：n就是num_generations\n",
    "        \n",
    "    # 2. 生成序列\n",
    "    with torch.no_grad():\n",
    "        batch_response, old_log_probs = policy_model.generate(\n",
    "            batch_prompt, max_len=max_len, return_log_probs=True\n",
    "        )  # batch_response: [batch_size * n, seq_len], old_log_probs: [batch_size * n, seq_len]\n",
    "        \n",
    "    response_mask = (batch_response != PAD_TOKEN_ID).float()  # 掩码：忽略 padding [batch_size * n, seq_len]\n",
    "    sep_token = torch.tensor([[SEP_TOKEN_ID]] * (batch_size * num_generations)).to(batch_prompt.device)\n",
    "    batch_data = torch.cat([batch_prompt, sep_token, batch_response], dim=1)  # [batch_size * n, len_prompt + 1 + seq_len]\n",
    "    \n",
    "    # 3. 奖励计算\n",
    "    # 过程监督：逐 token 奖励\n",
    "    batch_rewards_process = torch.zeros(batch_size * num_generations, max_len).to(device)\n",
    "    for w, reward_model in zip(weights, reward_models):\n",
    "        batch_rewards_process += w * reward_model(batch_data)  # [batch_size * n, seq_len]\n",
    "    batch_rewards_process = batch_rewards_process * response_mask  # 忽略 padding\n",
    "    \n",
    "    # 结果监督：序列级奖励\n",
    "    batch_rewards_outcome = torch.zeros(batch_size * num_generations).to(device)\n",
    "    for w, reward_model in zip(weights, reward_models):\n",
    "        batch_rewards_outcome += w * reward_model(batch_data, mode='outcome')  # [batch_size * n]\n",
    "        \n",
    "    # 4. 优势计算\n",
    "    # 过程监督---复杂度非常高\n",
    "    advantages_process = compute_process_advantages(\n",
    "        batch_rewards_process, batch_size, num_generations, max_len, gamma\n",
    "    )  # [batch_size * n, seq_len]\n",
    "    \n",
    "    # 结果监督---复杂度相对来说低很多\n",
    "    advantages_outcome = compute_outcome_advantages(\n",
    "        batch_rewards_outcome, batch_size, num_generations\n",
    "    )  # [batch_size * n, seq_len]\n",
    "    \n",
    "    # 选择优势函数（示例：使用过程监督）\n",
    "    advantages = advantages_process  # 或 advantages_outcome\n",
    "    \n",
    "    # 5. 参考模型前向传播\n",
    "    with torch.no_grad():\n",
    "        ref_logits = ref_model.forward(batch_data)  # [batch_size * n, seq_len, vocab_size]\n",
    "        ref_log_probs = gather_log_probs(ref_logits, batch_response)  # [batch_size * n, seq_len]\n",
    "        \n",
    "    # 6. 优化\n",
    "    for _ in range(mu):\n",
    "        # 新策略前向传播\n",
    "        new_logits = policy_model.forward(batch_data)  # [batch_size * n, seq_len, vocab_size]\n",
    "        new_log_probs = gather_log_probs(new_logits, batch_response)  # [batch_size * n, seq_len]\n",
    "        \n",
    "        # PPO 裁剪损失\n",
    "        ratio = torch.clamp(torch.exp(new_log_probs - old_log_probs.detach()), 1e-10, 1e10)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        loss_adv = -torch.mean(torch.min(surr1, surr2) * response_mask)  # 忽略 padding\n",
    "        \n",
    "        # KL 散度（标准方式）\n",
    "        loss_kl = compute_KL(new_logits, ref_logits, response_mask)\n",
    "        \n",
    "        # 总损失\n",
    "        loss = loss_adv + beta * loss_kl\n",
    "        \n",
    "        # 优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 7. 可选：打印损失\n",
    "    print(f\"Iteration {iteration}, Loss: {loss.item():.4f}, Adv Loss: {loss_adv.item():.4f}, KL Loss: {loss_kl.item():.4f}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
