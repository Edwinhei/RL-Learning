{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import indexOf\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from torch import log_softmax, sigmoid\n",
    "\n",
    "\"\"\"\n",
    "伪代码：DPO 训练流程\n",
    "目标：基于偏好数据集优化策略模型，使其更倾向于生成优选响应 y_w 而非拒绝响应 y_l\n",
    "输入：\n",
    "  - dataset: 偏好数据集，格式为 [(prompt, chosen_response, rejected_response)]\n",
    "  - policy_model: 策略模型 π_θ（如 Llama 3.1），待优化\n",
    "  - reference_model: 参考模型 π_ref（如 SFT 模型），通常固定\n",
    "  - beta: 温度参数，控制 DPO 损失尺度（通常 0.1–0.5）\n",
    "  - label_smoothing: 标签平滑参数，引入不确定性（默认 0.0 表示原始 DPO）\n",
    "  - reference_free: 是否忽略参考模型（默认 False）\n",
    "  - batch_size: 批次大小\n",
    "  - max_seq_len: 最大序列长度（例如 Llama 3.1 的 128k token）\n",
    "输出：\n",
    "  - losses: 每个样本的 DPO 损失\n",
    "  - chosen_rewards, rejected_rewards: 优选和拒绝响应的奖励值（用于监控）\n",
    "\"\"\"\n",
    "\n",
    "def dpo_training(dataset, policy_model, reference_model, beta=0.1, label_smoothing=0.0, reference_free=False, batch_size=32, max_seq_len=512):\n",
    "    \"\"\"DPO训练主函数，优化策略模型以对齐人类偏好\"\"\"\n",
    "    # 初始化优化器（例如 AdamW）\n",
    "    optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
    "    \n",
    "    # 初始化损失列表和奖励值\n",
    "    all_losses = []\n",
    "    all_chosen_rewards = []\n",
    "    all_rejected_rewards = []\n",
    "    # 按批次处理数据集\n",
    "    for batch_idx, batch in enumerate(dataset.batch_iter(batch_size)):\n",
    "        # 提取批次数据：提示、优选响应、拒绝响应\n",
    "        # 假设 dataset 已预处理为 token ID 格式\n",
    "        prompts, chosen_responses, rejected_responses = batch\n",
    "        # prompts: (batch_size, prompt_len)，token ID\n",
    "        # chosen_responses: (batch_size, chosen_len)，优选响应 token ID\n",
    "        # rejected_responses: (batch_size, rejected_len)，拒绝响应 token ID\n",
    "\n",
    "        # 创建掩码，标记有效 token（1 表示有效，0 表示填充）\n",
    "        # 假设序列已填充到 max_seq_len\n",
    "        prompt_mask = (prompts != pad_token_id).float()  # pad_token_id 由分词器定义\n",
    "        chosen_mask = (chosen_responses != pad_token_id).float()\n",
    "        rejected_mask = (rejected_responses != pad_token_id).float()\n",
    "\n",
    "        # 1. 计算对数概率（使用 Teaching Forcing）\n",
    "        # Teaching Forcing：输入真实序列 y[:t-1]，预测 y[t] 的概率\n",
    "        def compute_logprobs(model, prompt, response, mask):\n",
    "            \"\"\"\n",
    "            计算序列的对数概率。\n",
    "            Args:\n",
    "                model: 策略或参考模型\n",
    "                prompt: 输入提示，形状 (batch_size, prompt_len)\n",
    "                response: 目标响应，形状 (batch_size, response_len)\n",
    "                mask: 掩码，形状 (batch_size, response_len)\n",
    "            Returns:\n",
    "                logprobs: 每句话的对数概率和，形状 (batch_size,)\n",
    "            \"\"\"\n",
    "            # 拼接输入：prompt + response[:t-1]\n",
    "            # 假设模型支持拼接输入，实际中可能需分词器处理\n",
    "            input_ids = torch.cat([prompt, response[:, :-1]], dim=-1)\n",
    "            # input_ids: (batch_size, prompt_len + response_len - 1)\n",
    "\n",
    "            # 前向传播，获取 logits\n",
    "            # 因果掩码（Causal Mask）由 Transformer 自动应用，确保自回归\n",
    "            logits = model(input_ids)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # 位移操作：对齐 logits 和 labels\n",
    "            labels = response[:, 1:].clone()  # 去掉第一个 token（如 <bos>）\n",
    "            logits = logits[:, :-1, :]        # 去掉最后一个 logits\n",
    "\n",
    "            # 计算对数概率\n",
    "            logps = log_softmax(logits, dim=-1)  # (batch_size, seq_len-1, vocab_size)\n",
    "\n",
    "            # 提取目标 token 的对数概率\n",
    "            select_logprobs = torch.gather(\n",
    "                input=logps,\n",
    "                dim=-1,\n",
    "                index=labels.unsqueeze(-1)\n",
    "            ).squeeze(-1)  # (batch_size, seq_len-1)\n",
    "\n",
    "            # 应用掩码，忽略填充 token\n",
    "            mask = mask[:, 1:].clone()  # 对齐位移\n",
    "            select_logprobs = select_logprobs * mask\n",
    "            # 计算序列对数概率和（等价于概率连乘的对数）\n",
    "            logprobs = select_logprobs.sum(-1)  # (batch_size,)\n",
    "            # 归一化（可选）：除以有效 token 数量\n",
    "            # logprobs = select_logprobs.sum(-1) / mask.sum(-1)\n",
    "            return logprobs\n",
    "\n",
    "        # 计算策略模型的对数概率\n",
    "        policy_chosen_logps = compute_logprobs(\n",
    "            policy_model, prompts, chosen_responses, chosen_mask\n",
    "        )  # log π_θ(y_w|x)\n",
    "        policy_rejected_logps = compute_logprobs(\n",
    "            policy_model, prompts, rejected_responses, rejected_mask\n",
    "        )  # log π_θ(y_l|x)\n",
    "\n",
    "        # 计算参考模型的对数概率（若非 reference_free）\n",
    "        if reference_free:\n",
    "            reference_chosen_logps = torch.zeros_like(policy_chosen_logps)\n",
    "            reference_rejected_logps = torch.zeros_like(policy_rejected_logps)\n",
    "        else:\n",
    "            reference_chosen_logps = compute_logprobs(\n",
    "                reference_model, prompts, chosen_responses, chosen_mask\n",
    "            )  # log π_ref(y_w|x)\n",
    "            reference_rejected_logps = compute_logprobs(\n",
    "                reference_model, prompts, rejected_responses, rejected_mask\n",
    "            )  # log π_ref(y_l|x)\n",
    "\n",
    "        # 2. 计算 DPO 损失\n",
    "        # 对数概率比\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        # π_logratios = log (π_θ(y_w|x) / π_θ(y_l|x))\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "        # ref_logratios = log (π_ref(y_w|x) / π_ref(y_l|x))\n",
    "\n",
    "        # 逻辑值（Logits）\n",
    "        logits = pi_logratios - ref_logratios\n",
    "        # logits = log (π_θ(y_w|x) / π_θ(y_l|x)) - log (π_ref(y_w|x) / π_ref(y_l|x))\n",
    "\n",
    "        # Sigmoid 损失\n",
    "        losses = (\n",
    "            -F.logsigmoid(beta * logits) * (1 - label_smoothing)\n",
    "            - F.logsigmoid(-beta * logits) * label_smoothing\n",
    "        )\n",
    "        # losses: (batch_size,)，每个样本的 DPO 损失\n",
    "        # -log_sigmoid(z) = log(1 + e^{-z})，鼓励 logits > 0（即 y_w 更可能）\n",
    "\n",
    "        # 3. 计算奖励（用于监控，非必需）\n",
    "        # 奖励定义：r(x, y) = β * log (π_θ(y|x) / π_ref(y|x))\n",
    "        chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps)\n",
    "        rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps)\n",
    "\n",
    "        # 4. 优化\n",
    "        # 计算批量损失均值\n",
    "        loss = losses.mean()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 记录损失和奖励\n",
    "        all_losses.append(losses.detach())\n",
    "        all_chosen_rewards.append(chosen_rewards.detach())\n",
    "        all_rejected_rewards.append(rejected_rewards.detach())\n",
    "\n",
    "    # 返回所有损失和奖励\n",
    "    return torch.cat(all_losses), torch.cat(all_chosen_rewards), torch.cat(all_rejected_rewards)\n",
    "\n",
    "# 辅助函数：数据预处理（示例）\n",
    "def preprocess_batch(dataset, batch_size, max_seq_len, tokenizer):\n",
    "    \"\"\"\n",
    "    将原始数据转换为 token ID 和掩码。\n",
    "    Args:\n",
    "        dataset: 原始偏好数据集 [(prompt_text, chosen_text, rejected_text)]\n",
    "        batch_size: 批次大小\n",
    "        max_seq_len: 最大序列长度\n",
    "        tokenizer: 分词器（如 Llama 的 tokenizer）\n",
    "    Yields:\n",
    "        prompts, chosen_responses, rejected_responses: token ID 和掩码\n",
    "    \"\"\"\n",
    "    for batch in dataset.batch_iter(batch_size):\n",
    "        prompt_texts, chosen_texts, rejected_texts = zip(*batch)\n",
    "        \n",
    "        # 转换为 token ID\n",
    "        prompt_inputs = tokenizer(prompt_texts, padding=True, truncation=True, \n",
    "                                 max_length=max_seq_len, return_tensors=\"pt\")\n",
    "        chosen_inputs = tokenizer(chosen_texts, padding=True, truncation=True, \n",
    "                                  max_length=max_seq_len, return_tensors=\"pt\")\n",
    "        rejected_inputs = tokenizer(rejected_texts, padding=True, truncation=True, \n",
    "                                    max_length=max_seq_len, return_tensors=\"pt\")\n",
    "        \n",
    "        # 获取 token ID 和掩码\n",
    "        prompts = prompt_inputs[\"input_ids\"]  # (batch_size, prompt_len)\n",
    "        prompt_mask = prompt_inputs[\"attention_mask\"]  # (batch_size, prompt_len)\n",
    "        chosen_responses = chosen_inputs[\"input_ids\"]\n",
    "        chosen_mask = chosen_inputs[\"attention_mask\"]\n",
    "        rejected_responses = rejected_inputs[\"input_ids\"]\n",
    "        rejected_mask = rejected_inputs[\"attention_mask\"]\n",
    "\n",
    "        yield (prompts, chosen_responses, rejected_responses), \\\n",
    "              (prompt_mask, chosen_mask, rejected_mask)\n",
    "\n",
    "# 示例调用\n",
    "# 假设 dataset 是 [(prompt, chosen, rejected)] 格式\n",
    "# tokenizer 是 Llama 3.1 的分词器\n",
    "# policy_model 和 reference_model 是 Transformer 模型\n",
    "losses, chosen_rewards, rejected_rewards = dpo_training(\n",
    "    dataset=preprocess_batch(raw_dataset, batch_size=32, max_seq_len=512, tokenizer=tokenizer),\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    beta=0.1,\n",
    "    label_smoothing=0.0,\n",
    "    reference_free=False\n",
    ")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
