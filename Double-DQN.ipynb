{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf93179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58a1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 网络\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5365a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e36dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN 代理\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 主网络和目标网络\n",
    "        self.q_network = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        \n",
    "        # 超参数\n",
    "        self.gamma = 0.99  # 折扣因子\n",
    "        self.epsilon = 1.0  # 初始探索率\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.target_update_freq = 100  # 目标网络更新频率\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def update(self, step):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 从回放缓冲区采样\n",
    "        state, action, reward, next_state, done = self.memory.sample(self.batch_size)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = torch.LongTensor(action).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "\n",
    "        # 计算 Q 值（Double DQN）\n",
    "        q_values = self.q_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # 主网络选择动作\n",
    "        next_actions = self.q_network(next_state).argmax(1, keepdim=True)\n",
    "        # 目标网络评估动作\n",
    "        next_q_values = self.target_network(next_state).gather(1, next_actions).squeeze(1)\n",
    "        target = reward + (1 - done) * self.gamma * next_q_values\n",
    "        # 计算损失并更新网络\n",
    "        loss = nn.MSELoss()(q_values, target.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 更新探索率\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # 定期更新目标网络\n",
    "        if step % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def save_model(self, path=\"double_dqn_cartpole.pth\"):\n",
    "        torch.save(self.q_network.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path=\"double_dqn_cartpole.pth\"):\n",
    "        self.q_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# 训练函数\n",
    "def train_double_dqn():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = DoubleDQNAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
    "    \n",
    "    num_episodes = 500\n",
    "    max_steps = 500\n",
    "    scores = []\n",
    "    step = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            agent.update(step)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Reward: {episode_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    agent.save_model()\n",
    "    \n",
    "    # 绘制训练过程中的奖励曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Training Rewards Over Episodes (Double DQN)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"double_dqn_training_rewards.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return agent\n",
    "\n",
    "# 测试和可视化\n",
    "def test_double_dqn(agent):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    agent.epsilon = 0.0  # 测试时禁用探索\n",
    "\n",
    "    num_test_episodes = 5\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < 500:\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            episode_reward += reward\n",
    "            t += 1\n",
    "\n",
    "        test_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode+1}, Reward: {episode_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    return test_rewards\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1742d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练 Double DQN...\n",
      "Episode 1/500, Reward: 12.0, Epsilon: 1.000\n",
      "Episode 2/500, Reward: 16.0, Epsilon: 1.000\n",
      "Episode 3/500, Reward: 12.0, Epsilon: 1.000\n",
      "Episode 4/500, Reward: 13.0, Epsilon: 1.000\n",
      "Episode 5/500, Reward: 22.0, Epsilon: 0.942\n",
      "Episode 6/500, Reward: 16.0, Epsilon: 0.869\n",
      "Episode 7/500, Reward: 37.0, Epsilon: 0.722\n",
      "Episode 8/500, Reward: 20.0, Epsilon: 0.653\n",
      "Episode 9/500, Reward: 10.0, Epsilon: 0.621\n",
      "Episode 10/500, Reward: 9.0, Epsilon: 0.594\n",
      "Episode 11/500, Reward: 22.0, Epsilon: 0.532\n",
      "Episode 12/500, Reward: 11.0, Epsilon: 0.503\n",
      "Episode 13/500, Reward: 11.0, Epsilon: 0.476\n",
      "Episode 14/500, Reward: 17.0, Epsilon: 0.437\n",
      "Episode 15/500, Reward: 11.0, Epsilon: 0.414\n",
      "Episode 16/500, Reward: 9.0, Epsilon: 0.396\n",
      "Episode 17/500, Reward: 14.0, Epsilon: 0.369\n",
      "Episode 18/500, Reward: 9.0, Epsilon: 0.353\n",
      "Episode 19/500, Reward: 11.0, Epsilon: 0.334\n",
      "Episode 20/500, Reward: 9.0, Epsilon: 0.319\n",
      "Episode 21/500, Reward: 14.0, Epsilon: 0.297\n",
      "Episode 22/500, Reward: 9.0, Epsilon: 0.284\n",
      "Episode 23/500, Reward: 14.0, Epsilon: 0.265\n",
      "Episode 24/500, Reward: 16.0, Epsilon: 0.245\n",
      "Episode 25/500, Reward: 13.0, Epsilon: 0.229\n",
      "Episode 26/500, Reward: 43.0, Epsilon: 0.185\n",
      "Episode 27/500, Reward: 21.0, Epsilon: 0.166\n",
      "Episode 28/500, Reward: 18.0, Epsilon: 0.152\n",
      "Episode 29/500, Reward: 29.0, Epsilon: 0.131\n",
      "Episode 30/500, Reward: 24.0, Epsilon: 0.116\n",
      "Episode 31/500, Reward: 21.0, Epsilon: 0.105\n",
      "Episode 32/500, Reward: 18.0, Epsilon: 0.096\n",
      "Episode 33/500, Reward: 28.0, Epsilon: 0.083\n",
      "Episode 34/500, Reward: 18.0, Epsilon: 0.076\n",
      "Episode 35/500, Reward: 26.0, Epsilon: 0.067\n",
      "Episode 36/500, Reward: 39.0, Epsilon: 0.055\n",
      "Episode 37/500, Reward: 44.0, Epsilon: 0.044\n",
      "Episode 38/500, Reward: 85.0, Epsilon: 0.029\n",
      "Episode 39/500, Reward: 151.0, Epsilon: 0.013\n",
      "Episode 40/500, Reward: 237.0, Epsilon: 0.010\n",
      "Episode 41/500, Reward: 248.0, Epsilon: 0.010\n",
      "Episode 42/500, Reward: 235.0, Epsilon: 0.010\n",
      "Episode 43/500, Reward: 274.0, Epsilon: 0.010\n",
      "Episode 44/500, Reward: 199.0, Epsilon: 0.010\n",
      "Episode 45/500, Reward: 255.0, Epsilon: 0.010\n",
      "Episode 46/500, Reward: 200.0, Epsilon: 0.010\n",
      "Episode 47/500, Reward: 472.0, Epsilon: 0.010\n",
      "Episode 48/500, Reward: 312.0, Epsilon: 0.010\n",
      "Episode 49/500, Reward: 177.0, Epsilon: 0.010\n",
      "Episode 50/500, Reward: 197.0, Epsilon: 0.010\n",
      "Episode 51/500, Reward: 201.0, Epsilon: 0.010\n",
      "Episode 52/500, Reward: 204.0, Epsilon: 0.010\n",
      "Episode 53/500, Reward: 307.0, Epsilon: 0.010\n",
      "Episode 54/500, Reward: 199.0, Epsilon: 0.010\n",
      "Episode 55/500, Reward: 363.0, Epsilon: 0.010\n",
      "Episode 56/500, Reward: 294.0, Epsilon: 0.010\n",
      "Episode 57/500, Reward: 199.0, Epsilon: 0.010\n",
      "Episode 58/500, Reward: 265.0, Epsilon: 0.010\n",
      "Episode 59/500, Reward: 223.0, Epsilon: 0.010\n",
      "Episode 60/500, Reward: 219.0, Epsilon: 0.010\n",
      "Episode 61/500, Reward: 346.0, Epsilon: 0.010\n",
      "Episode 62/500, Reward: 228.0, Epsilon: 0.010\n",
      "Episode 63/500, Reward: 180.0, Epsilon: 0.010\n",
      "Episode 64/500, Reward: 199.0, Epsilon: 0.010\n",
      "Episode 65/500, Reward: 229.0, Epsilon: 0.010\n",
      "Episode 66/500, Reward: 252.0, Epsilon: 0.010\n",
      "Episode 67/500, Reward: 425.0, Epsilon: 0.010\n",
      "Episode 68/500, Reward: 179.0, Epsilon: 0.010\n",
      "Episode 69/500, Reward: 206.0, Epsilon: 0.010\n",
      "Episode 70/500, Reward: 219.0, Epsilon: 0.010\n",
      "Episode 71/500, Reward: 252.0, Epsilon: 0.010\n",
      "Episode 72/500, Reward: 199.0, Epsilon: 0.010\n",
      "Episode 73/500, Reward: 308.0, Epsilon: 0.010\n",
      "Episode 74/500, Reward: 195.0, Epsilon: 0.010\n",
      "Episode 75/500, Reward: 178.0, Epsilon: 0.010\n",
      "Episode 76/500, Reward: 227.0, Epsilon: 0.010\n",
      "Episode 77/500, Reward: 194.0, Epsilon: 0.010\n",
      "Episode 78/500, Reward: 188.0, Epsilon: 0.010\n",
      "Episode 79/500, Reward: 206.0, Epsilon: 0.010\n",
      "Episode 80/500, Reward: 185.0, Epsilon: 0.010\n",
      "Episode 81/500, Reward: 198.0, Epsilon: 0.010\n",
      "Episode 82/500, Reward: 207.0, Epsilon: 0.010\n",
      "Episode 83/500, Reward: 349.0, Epsilon: 0.010\n",
      "Episode 84/500, Reward: 282.0, Epsilon: 0.010\n",
      "Episode 85/500, Reward: 348.0, Epsilon: 0.010\n",
      "Episode 86/500, Reward: 228.0, Epsilon: 0.010\n",
      "Episode 87/500, Reward: 272.0, Epsilon: 0.010\n",
      "Episode 88/500, Reward: 209.0, Epsilon: 0.010\n",
      "Episode 89/500, Reward: 254.0, Epsilon: 0.010\n",
      "Episode 90/500, Reward: 257.0, Epsilon: 0.010\n",
      "Episode 91/500, Reward: 222.0, Epsilon: 0.010\n",
      "Episode 92/500, Reward: 289.0, Epsilon: 0.010\n",
      "Episode 93/500, Reward: 300.0, Epsilon: 0.010\n",
      "Episode 94/500, Reward: 218.0, Epsilon: 0.010\n",
      "Episode 95/500, Reward: 237.0, Epsilon: 0.010\n",
      "Episode 96/500, Reward: 300.0, Epsilon: 0.010\n",
      "Episode 97/500, Reward: 334.0, Epsilon: 0.010\n",
      "Episode 98/500, Reward: 340.0, Epsilon: 0.010\n",
      "Episode 99/500, Reward: 320.0, Epsilon: 0.010\n",
      "Episode 100/500, Reward: 291.0, Epsilon: 0.010\n",
      "Episode 101/500, Reward: 364.0, Epsilon: 0.010\n",
      "Episode 102/500, Reward: 321.0, Epsilon: 0.010\n",
      "Episode 103/500, Reward: 378.0, Epsilon: 0.010\n",
      "Episode 104/500, Reward: 404.0, Epsilon: 0.010\n",
      "Episode 105/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 106/500, Reward: 391.0, Epsilon: 0.010\n",
      "Episode 107/500, Reward: 324.0, Epsilon: 0.010\n",
      "Episode 108/500, Reward: 299.0, Epsilon: 0.010\n",
      "Episode 109/500, Reward: 287.0, Epsilon: 0.010\n",
      "Episode 110/500, Reward: 321.0, Epsilon: 0.010\n",
      "Episode 111/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 112/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 113/500, Reward: 284.0, Epsilon: 0.010\n",
      "Episode 114/500, Reward: 263.0, Epsilon: 0.010\n",
      "Episode 115/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 116/500, Reward: 359.0, Epsilon: 0.010\n",
      "Episode 117/500, Reward: 439.0, Epsilon: 0.010\n",
      "Episode 118/500, Reward: 360.0, Epsilon: 0.010\n",
      "Episode 119/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 120/500, Reward: 325.0, Epsilon: 0.010\n",
      "Episode 121/500, Reward: 312.0, Epsilon: 0.010\n",
      "Episode 122/500, Reward: 344.0, Epsilon: 0.010\n",
      "Episode 123/500, Reward: 177.0, Epsilon: 0.010\n",
      "Episode 124/500, Reward: 385.0, Epsilon: 0.010\n",
      "Episode 125/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 126/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 127/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 128/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 129/500, Reward: 269.0, Epsilon: 0.010\n",
      "Episode 130/500, Reward: 113.0, Epsilon: 0.010\n",
      "Episode 131/500, Reward: 183.0, Epsilon: 0.010\n",
      "Episode 132/500, Reward: 189.0, Epsilon: 0.010\n",
      "Episode 133/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 134/500, Reward: 294.0, Epsilon: 0.010\n",
      "Episode 135/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 136/500, Reward: 380.0, Epsilon: 0.010\n",
      "Episode 137/500, Reward: 372.0, Epsilon: 0.010\n",
      "Episode 138/500, Reward: 336.0, Epsilon: 0.010\n",
      "Episode 139/500, Reward: 311.0, Epsilon: 0.010\n",
      "Episode 140/500, Reward: 341.0, Epsilon: 0.010\n",
      "Episode 141/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 142/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 143/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 144/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 145/500, Reward: 401.0, Epsilon: 0.010\n",
      "Episode 146/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 147/500, Reward: 51.0, Epsilon: 0.010\n",
      "Episode 148/500, Reward: 210.0, Epsilon: 0.010\n",
      "Episode 149/500, Reward: 182.0, Epsilon: 0.010\n",
      "Episode 150/500, Reward: 217.0, Epsilon: 0.010\n",
      "Episode 151/500, Reward: 253.0, Epsilon: 0.010\n",
      "Episode 152/500, Reward: 253.0, Epsilon: 0.010\n",
      "Episode 153/500, Reward: 324.0, Epsilon: 0.010\n",
      "Episode 154/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 155/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 156/500, Reward: 319.0, Epsilon: 0.010\n",
      "Episode 157/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 158/500, Reward: 21.0, Epsilon: 0.010\n",
      "Episode 159/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 160/500, Reward: 15.0, Epsilon: 0.010\n",
      "Episode 161/500, Reward: 119.0, Epsilon: 0.010\n",
      "Episode 162/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 163/500, Reward: 126.0, Epsilon: 0.010\n",
      "Episode 164/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 165/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 166/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 167/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 168/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 169/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 170/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 171/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 172/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 173/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 174/500, Reward: 427.0, Epsilon: 0.010\n",
      "Episode 175/500, Reward: 350.0, Epsilon: 0.010\n",
      "Episode 176/500, Reward: 240.0, Epsilon: 0.010\n",
      "Episode 177/500, Reward: 225.0, Epsilon: 0.010\n",
      "Episode 178/500, Reward: 211.0, Epsilon: 0.010\n",
      "Episode 179/500, Reward: 260.0, Epsilon: 0.010\n",
      "Episode 180/500, Reward: 236.0, Epsilon: 0.010\n",
      "Episode 181/500, Reward: 324.0, Epsilon: 0.010\n",
      "Episode 182/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 183/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 184/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 185/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 186/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 187/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 188/500, Reward: 369.0, Epsilon: 0.010\n",
      "Episode 189/500, Reward: 323.0, Epsilon: 0.010\n",
      "Episode 190/500, Reward: 439.0, Epsilon: 0.010\n",
      "Episode 191/500, Reward: 489.0, Epsilon: 0.010\n",
      "Episode 192/500, Reward: 454.0, Epsilon: 0.010\n",
      "Episode 193/500, Reward: 256.0, Epsilon: 0.010\n",
      "Episode 194/500, Reward: 186.0, Epsilon: 0.010\n",
      "Episode 195/500, Reward: 80.0, Epsilon: 0.010\n",
      "Episode 196/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 197/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 198/500, Reward: 313.0, Epsilon: 0.010\n",
      "Episode 199/500, Reward: 315.0, Epsilon: 0.010\n",
      "Episode 200/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 201/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 202/500, Reward: 440.0, Epsilon: 0.010\n",
      "Episode 203/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 204/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 205/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 206/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 207/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 208/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 209/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 210/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 211/500, Reward: 422.0, Epsilon: 0.010\n",
      "Episode 212/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 213/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 214/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 215/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 216/500, Reward: 363.0, Epsilon: 0.010\n",
      "Episode 217/500, Reward: 484.0, Epsilon: 0.010\n",
      "Episode 218/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 219/500, Reward: 213.0, Epsilon: 0.010\n",
      "Episode 220/500, Reward: 76.0, Epsilon: 0.010\n",
      "Episode 221/500, Reward: 55.0, Epsilon: 0.010\n",
      "Episode 222/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 223/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 224/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 225/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 226/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 227/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 228/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 229/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 230/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 231/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 232/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 233/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 234/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 235/500, Reward: 45.0, Epsilon: 0.010\n",
      "Episode 236/500, Reward: 46.0, Epsilon: 0.010\n",
      "Episode 237/500, Reward: 119.0, Epsilon: 0.010\n",
      "Episode 238/500, Reward: 119.0, Epsilon: 0.010\n",
      "Episode 239/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 240/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 241/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 242/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 243/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 244/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 245/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 246/500, Reward: 438.0, Epsilon: 0.010\n",
      "Episode 247/500, Reward: 151.0, Epsilon: 0.010\n",
      "Episode 248/500, Reward: 291.0, Epsilon: 0.010\n",
      "Episode 249/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 250/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 251/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 252/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 253/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 254/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 255/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 256/500, Reward: 402.0, Epsilon: 0.010\n",
      "Episode 257/500, Reward: 447.0, Epsilon: 0.010\n",
      "Episode 258/500, Reward: 342.0, Epsilon: 0.010\n",
      "Episode 259/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 260/500, Reward: 382.0, Epsilon: 0.010\n",
      "Episode 261/500, Reward: 180.0, Epsilon: 0.010\n",
      "Episode 262/500, Reward: 198.0, Epsilon: 0.010\n",
      "Episode 263/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 264/500, Reward: 363.0, Epsilon: 0.010\n",
      "Episode 265/500, Reward: 163.0, Epsilon: 0.010\n",
      "Episode 266/500, Reward: 151.0, Epsilon: 0.010\n",
      "Episode 267/500, Reward: 238.0, Epsilon: 0.010\n",
      "Episode 268/500, Reward: 297.0, Epsilon: 0.010\n",
      "Episode 269/500, Reward: 254.0, Epsilon: 0.010\n",
      "Episode 270/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 271/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 272/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 273/500, Reward: 128.0, Epsilon: 0.010\n",
      "Episode 274/500, Reward: 448.0, Epsilon: 0.010\n",
      "Episode 275/500, Reward: 306.0, Epsilon: 0.010\n",
      "Episode 276/500, Reward: 188.0, Epsilon: 0.010\n",
      "Episode 277/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 278/500, Reward: 363.0, Epsilon: 0.010\n",
      "Episode 279/500, Reward: 14.0, Epsilon: 0.010\n",
      "Episode 280/500, Reward: 12.0, Epsilon: 0.010\n",
      "Episode 281/500, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 282/500, Reward: 115.0, Epsilon: 0.010\n",
      "Episode 283/500, Reward: 102.0, Epsilon: 0.010\n",
      "Episode 284/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 285/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 286/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 287/500, Reward: 327.0, Epsilon: 0.010\n",
      "Episode 288/500, Reward: 257.0, Epsilon: 0.010\n",
      "Episode 289/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 290/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 291/500, Reward: 226.0, Epsilon: 0.010\n",
      "Episode 292/500, Reward: 226.0, Epsilon: 0.010\n",
      "Episode 293/500, Reward: 215.0, Epsilon: 0.010\n",
      "Episode 294/500, Reward: 329.0, Epsilon: 0.010\n",
      "Episode 295/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 296/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 297/500, Reward: 226.0, Epsilon: 0.010\n",
      "Episode 298/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 299/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 300/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 301/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 302/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 303/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 304/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 305/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 306/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 307/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 308/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 309/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 310/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 311/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 312/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 313/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 314/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 315/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 316/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 317/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 318/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 319/500, Reward: 216.0, Epsilon: 0.010\n",
      "Episode 320/500, Reward: 246.0, Epsilon: 0.010\n",
      "Episode 321/500, Reward: 295.0, Epsilon: 0.010\n",
      "Episode 322/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 323/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 324/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 325/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 326/500, Reward: 451.0, Epsilon: 0.010\n",
      "Episode 327/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 328/500, Reward: 455.0, Epsilon: 0.010\n",
      "Episode 329/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 330/500, Reward: 276.0, Epsilon: 0.010\n",
      "Episode 331/500, Reward: 228.0, Epsilon: 0.010\n",
      "Episode 332/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 333/500, Reward: 271.0, Epsilon: 0.010\n",
      "Episode 334/500, Reward: 288.0, Epsilon: 0.010\n",
      "Episode 335/500, Reward: 234.0, Epsilon: 0.010\n",
      "Episode 336/500, Reward: 241.0, Epsilon: 0.010\n",
      "Episode 337/500, Reward: 349.0, Epsilon: 0.010\n",
      "Episode 338/500, Reward: 252.0, Epsilon: 0.010\n",
      "Episode 339/500, Reward: 241.0, Epsilon: 0.010\n",
      "Episode 340/500, Reward: 183.0, Epsilon: 0.010\n",
      "Episode 341/500, Reward: 155.0, Epsilon: 0.010\n",
      "Episode 342/500, Reward: 255.0, Epsilon: 0.010\n",
      "Episode 343/500, Reward: 306.0, Epsilon: 0.010\n",
      "Episode 344/500, Reward: 204.0, Epsilon: 0.010\n",
      "Episode 345/500, Reward: 188.0, Epsilon: 0.010\n",
      "Episode 346/500, Reward: 166.0, Epsilon: 0.010\n",
      "Episode 347/500, Reward: 229.0, Epsilon: 0.010\n",
      "Episode 348/500, Reward: 257.0, Epsilon: 0.010\n",
      "Episode 349/500, Reward: 339.0, Epsilon: 0.010\n",
      "Episode 350/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 351/500, Reward: 241.0, Epsilon: 0.010\n",
      "Episode 352/500, Reward: 280.0, Epsilon: 0.010\n",
      "Episode 353/500, Reward: 255.0, Epsilon: 0.010\n",
      "Episode 354/500, Reward: 133.0, Epsilon: 0.010\n",
      "Episode 355/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 356/500, Reward: 458.0, Epsilon: 0.010\n",
      "Episode 357/500, Reward: 226.0, Epsilon: 0.010\n",
      "Episode 358/500, Reward: 235.0, Epsilon: 0.010\n",
      "Episode 359/500, Reward: 262.0, Epsilon: 0.010\n",
      "Episode 360/500, Reward: 261.0, Epsilon: 0.010\n",
      "Episode 361/500, Reward: 313.0, Epsilon: 0.010\n",
      "Episode 362/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 363/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 364/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 365/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 366/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 367/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 368/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 369/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 370/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 371/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 372/500, Reward: 400.0, Epsilon: 0.010\n",
      "Episode 373/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 374/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 375/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 376/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 377/500, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 378/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 379/500, Reward: 102.0, Epsilon: 0.010\n",
      "Episode 380/500, Reward: 292.0, Epsilon: 0.010\n",
      "Episode 381/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 382/500, Reward: 452.0, Epsilon: 0.010\n",
      "Episode 383/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 384/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 385/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 386/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 387/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 388/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 389/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 390/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 391/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 392/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 393/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 394/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 395/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 396/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 397/500, Reward: 202.0, Epsilon: 0.010\n",
      "Episode 398/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 399/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 400/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 401/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 402/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 403/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 404/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 405/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 406/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 407/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 408/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 409/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 410/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 411/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 412/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 413/500, Reward: 17.0, Epsilon: 0.010\n",
      "Episode 414/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 415/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 416/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 417/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 418/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 419/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 420/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 421/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 422/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 423/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 424/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 425/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 426/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 427/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 428/500, Reward: 164.0, Epsilon: 0.010\n",
      "Episode 429/500, Reward: 49.0, Epsilon: 0.010\n",
      "Episode 430/500, Reward: 30.0, Epsilon: 0.010\n",
      "Episode 431/500, Reward: 68.0, Epsilon: 0.010\n",
      "Episode 432/500, Reward: 125.0, Epsilon: 0.010\n",
      "Episode 433/500, Reward: 31.0, Epsilon: 0.010\n",
      "Episode 434/500, Reward: 29.0, Epsilon: 0.010\n",
      "Episode 435/500, Reward: 156.0, Epsilon: 0.010\n",
      "Episode 436/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 437/500, Reward: 182.0, Epsilon: 0.010\n",
      "Episode 438/500, Reward: 168.0, Epsilon: 0.010\n",
      "Episode 439/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 440/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 441/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 442/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 443/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 444/500, Reward: 320.0, Epsilon: 0.010\n",
      "Episode 445/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 446/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 447/500, Reward: 235.0, Epsilon: 0.010\n",
      "Episode 448/500, Reward: 220.0, Epsilon: 0.010\n",
      "Episode 449/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 450/500, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 451/500, Reward: 153.0, Epsilon: 0.010\n",
      "Episode 452/500, Reward: 194.0, Epsilon: 0.010\n",
      "Episode 453/500, Reward: 137.0, Epsilon: 0.010\n",
      "Episode 454/500, Reward: 139.0, Epsilon: 0.010\n",
      "Episode 455/500, Reward: 136.0, Epsilon: 0.010\n",
      "Episode 456/500, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 457/500, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 458/500, Reward: 142.0, Epsilon: 0.010\n",
      "Episode 459/500, Reward: 471.0, Epsilon: 0.010\n",
      "Episode 460/500, Reward: 134.0, Epsilon: 0.010\n",
      "Episode 461/500, Reward: 146.0, Epsilon: 0.010\n",
      "Episode 462/500, Reward: 170.0, Epsilon: 0.010\n",
      "Episode 463/500, Reward: 156.0, Epsilon: 0.010\n",
      "Episode 464/500, Reward: 148.0, Epsilon: 0.010\n",
      "Episode 465/500, Reward: 165.0, Epsilon: 0.010\n",
      "Episode 466/500, Reward: 152.0, Epsilon: 0.010\n",
      "Episode 467/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 468/500, Reward: 167.0, Epsilon: 0.010\n",
      "Episode 469/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 470/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 471/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 472/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 473/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 474/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 475/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 476/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 477/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 478/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 479/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 480/500, Reward: 297.0, Epsilon: 0.010\n",
      "Episode 481/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 482/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 483/500, Reward: 282.0, Epsilon: 0.010\n",
      "Episode 484/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 485/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 486/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 487/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 488/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 489/500, Reward: 500.0, Epsilon: 0.010\n",
      "Episode 490/500, Reward: 289.0, Epsilon: 0.010\n",
      "Episode 491/500, Reward: 358.0, Epsilon: 0.010\n",
      "Episode 492/500, Reward: 262.0, Epsilon: 0.010\n",
      "Episode 493/500, Reward: 221.0, Epsilon: 0.010\n",
      "Episode 494/500, Reward: 275.0, Epsilon: 0.010\n",
      "Episode 495/500, Reward: 338.0, Epsilon: 0.010\n",
      "Episode 496/500, Reward: 291.0, Epsilon: 0.010\n",
      "Episode 497/500, Reward: 289.0, Epsilon: 0.010\n",
      "Episode 498/500, Reward: 272.0, Epsilon: 0.010\n",
      "Episode 499/500, Reward: 270.0, Epsilon: 0.010\n",
      "Episode 500/500, Reward: 347.0, Epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "print(\"开始训练 Double DQN...\")\n",
    "agent = train_double_dqn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0434d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始测试和可视化...\n",
      "Test Episode 1, Reward: 388.0\n",
      "Test Episode 2, Reward: 376.0\n",
      "Test Episode 3, Reward: 391.0\n",
      "Test Episode 4, Reward: 378.0\n",
      "Test Episode 5, Reward: 377.0\n",
      "\n",
      "模型评估：\n",
      "平均测试奖励：382.00\n",
      "训练奖励曲线已保存为 'double_dqn_training_rewards.png'\n",
      "表现良好！代理能够保持平衡一段时间，但还有改进空间。\n"
     ]
    }
   ],
   "source": [
    "# 测试和可视化\n",
    "print(\"\\n开始测试和可视化...\")\n",
    "test_rewards = test_double_dqn(agent)\n",
    "\n",
    "# 评估表现\n",
    "avg_reward = np.mean(test_rewards)\n",
    "print(f\"\\n模型评估：\")\n",
    "print(f\"平均测试奖励：{avg_reward:.2f}\")\n",
    "print(\"训练奖励曲线已保存为 'double_dqn_training_rewards.png'\")\n",
    "if avg_reward > 475:\n",
    "    print(\"表现优秀！代理成功学会了保持杆子平衡，接近最大奖励 500。\")\n",
    "elif avg_reward > 300:\n",
    "    print(\"表现良好！代理能够保持平衡一段时间，但还有改进空间。\")\n",
    "else:\n",
    "    print(\"表现一般。代理需要更多训练以提升性能。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f58638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
