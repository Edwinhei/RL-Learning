{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个Actor-Critic代码实现基于Monte Carlo回报的。\n",
    "\n",
    "# 导入依赖库\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# 检查CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义Actor网络\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 定义Critic网络\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Actor-Critic代理类\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, env_name=\"Acrobot-v1\", hidden_dim=256, lr_actor=0.0005, lr_critic=0.001):\n",
    "        # 初始化环境\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        \n",
    "        # 初始化独立网络\n",
    "        self.actor = ActorNetwork(self.state_dim, hidden_dim, self.action_dim).to(device)\n",
    "        self.critic = CriticNetwork(self.state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        # 存储训练数据\n",
    "        self.rewards_history = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "    \n",
    "    def save_checkpoint(self, episode, filename_prefix=\"acrobot_ac_checkpoint\"):\n",
    "        # 保存检查点\n",
    "        filename = f\"{filename_prefix}_{episode}.pth\"\n",
    "        checkpoint = {\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optimizer': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer': self.critic_optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        print(f\"Saved checkpoint: {filename}\")\n",
    "    \n",
    "    def save_model(self, filename=\"acrobot_ac_final.pth\"):\n",
    "        # 保存最终模型\n",
    "        checkpoint = {\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optimizer': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer': self.critic_optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        print(f\"Final model weights saved to {filename}\")\n",
    "    \n",
    "    def load_model(self, filename=\"acrobot_ac_final.pth\"):\n",
    "        # 加载模型权重\n",
    "        try:\n",
    "            checkpoint = torch.load(filename, map_location=device)\n",
    "            self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "            print(f\"Loaded model weights: {filename}\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model weights file {filename} not found\")\n",
    "            return False\n",
    "    \n",
    "    def train(self, n_episodes=2000, gamma=0.99):\n",
    "        # 训练Actor-Critic\n",
    "        print(\"Starting training...\")\n",
    "        self.rewards_history = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            done = False\n",
    "            \n",
    "            # 收集轨迹\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).to(device)\n",
    "                action_probs = self.actor(state_tensor)\n",
    "                value = self.critic(state_tensor)\n",
    "                m = Categorical(action_probs)\n",
    "                action = m.sample()\n",
    "                log_prob = m.log_prob(action)\n",
    "                \n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                rewards.append(reward)\n",
    "                state = next_state\n",
    "            \n",
    "            # 计算回报\n",
    "            returns = []\n",
    "            R = 0\n",
    "            for r in rewards[::-1]:\n",
    "                R = r + gamma * R\n",
    "                returns.insert(0, R)\n",
    "            returns = torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "            \n",
    "            # 计算Actor和Critic损失\n",
    "            actor_loss = 0\n",
    "            critic_loss = 0\n",
    "            for log_prob, value, R in zip(log_probs, values, returns):\n",
    "                # 计算单步优势\n",
    "                advantage = R - value.item()\n",
    "                # 使用单步优势，计算actor损失\n",
    "                actor_loss += -log_prob * advantage\n",
    "                # 评估同一状态下，实际回报值和Critic预测状态值之间的损失\n",
    "                critic_loss += (R - value) ** 2\n",
    "            \n",
    "            actor_loss = actor_loss.mean()\n",
    "            critic_loss = critic_loss.mean()\n",
    "            \n",
    "            # 更新Actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            # 更新Critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            total_reward = sum(rewards)\n",
    "            self.rewards_history.append(total_reward)\n",
    "            self.actor_losses.append(actor_loss.item())\n",
    "            self.critic_losses.append(critic_loss.item())\n",
    "            \n",
    "            # 打印训练数据\n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Episode {episode:4d}: Reward = {total_reward:6.2f}, \"\n",
    "                      f\"Avg Reward (last 50) = {np.mean(self.rewards_history[-50:]):6.2f}, \"\n",
    "                      f\"Actor Loss = {actor_loss.item():.4f}, Critic Loss = {critic_loss.item():.4f}\")\n",
    "            \n",
    "            # 保存检查点\n",
    "            if episode % 500 == 0 and episode > 0:\n",
    "                self.save_checkpoint(episode)\n",
    "        \n",
    "        # 保存最终模型\n",
    "        self.save_model()\n",
    "    \n",
    "    def plot_training_results(self):\n",
    "        # 绘制训练结果（所有文本用英文）\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.rewards_history, label=\"Episode Reward\", alpha=0.5)\n",
    "        plt.plot(np.convolve(self.rewards_history, np.ones(100)/100, mode='valid'), \n",
    "                 label=\"Moving Avg (100)\", color='red')\n",
    "        plt.title(\"Training Reward Curve\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Total Reward\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.actor_losses, label=\"Actor Loss\", alpha=0.5)\n",
    "        plt.plot(np.convolve(self.actor_losses, np.ones(100)/100, mode='valid'), \n",
    "                 label=\"Moving Avg (100)\", color='red')\n",
    "        plt.title(\"Actor Loss Curve\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.critic_losses, label=\"Critic Loss\", alpha=0.5)\n",
    "        plt.plot(np.convolve(self.critic_losses, np.ones(100)/100, mode='valid'), \n",
    "                 label=\"Moving Avg (100)\", color='red')\n",
    "        plt.title(\"Critic Loss Curve\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"training_results.png\")\n",
    "        plt.show()\n",
    "        print(\"Training results saved as training_results.png\")\n",
    "    \n",
    "    def evaluate_training(self):\n",
    "        # 评估训练表现\n",
    "        avg_train_reward = np.mean(self.rewards_history[-100:])\n",
    "        print(f\"\\nTraining Evaluation:\")\n",
    "        print(f\"Average Reward (Last 100 Episodes): {avg_train_reward:.2f}\")\n",
    "        if avg_train_reward > -100:\n",
    "            print(\"Performance: Excellent - Solved Acrobot (threshold > -100)\")\n",
    "        elif avg_train_reward > -200:\n",
    "            print(\"Performance: Good - Stable but not optimal\")\n",
    "        else:\n",
    "            print(\"Performance: Needs improvement\")\n",
    "    \n",
    "    def test(self, num_episodes=5):\n",
    "        # 测试与可视化\n",
    "        test_env = gym.make(\"Acrobot-v1\", render_mode=\"human\")\n",
    "        if not self.load_model():\n",
    "            return []\n",
    "        \n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "        episode_rewards = []\n",
    "        print(\"\\nStarting testing...\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = test_env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            test_env.render()\n",
    "            print(f\"\\nTest Episode {episode}:\")\n",
    "            print(\"Step | State (cosθ1, sinθ1, cosθ2, sinθ2) | Action | Prob Left | Prob None | Prob Right | Reward | State Value\")\n",
    "            print(\"-\" * 110)\n",
    "            step = 0\n",
    "            \n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).to(device)\n",
    "                action_probs = self.actor(state_tensor)\n",
    "                value = self.critic(state_tensor)\n",
    "                m = Categorical(action_probs)\n",
    "                action = m.sample()\n",
    "                prob_left = action_probs[0].item()\n",
    "                prob_none = action_probs[1].item()\n",
    "                prob_right = action_probs[2].item()\n",
    "                \n",
    "                state, reward, terminated, truncated, _ = test_env.step(action.item())\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "                \n",
    "                print(f\"{step:4d} | {state[0]:.3f}, {state[1]:.3f}, {state[2]:.3f}, {state[3]:.3f} | \"\n",
    "                      f\"{action.item()} | {prob_left:.3f} | {prob_none:.3f} | {prob_right:.3f} | \"\n",
    "                      f\"{reward:.2f} | {value.item():.2f}\")\n",
    "                step += 1\n",
    "                test_env.render()\n",
    "            \n",
    "            episode_rewards.append(total_reward)\n",
    "            print(f\"Test Episode {episode} Total Reward: {total_reward:.2f}\")\n",
    "        \n",
    "        test_env.close()\n",
    "        return episode_rewards\n",
    "    \n",
    "    def evaluate_testing(self, test_rewards):\n",
    "        # 评估测试表现\n",
    "        if test_rewards:\n",
    "            avg_test_reward = np.mean(test_rewards)\n",
    "            print(f\"\\nTest Evaluation:\")\n",
    "            print(f\"Average Reward (Test Episodes): {avg_test_reward:.2f}\")\n",
    "            if avg_test_reward > -100:\n",
    "                print(\"Performance: Excellent - Solved Acrobot (threshold > -100)\")\n",
    "            elif avg_test_reward > -200:\n",
    "                print(\"Performance: Good - Stable but not optimal\")\n",
    "            else:\n",
    "                print(\"Performance: Needs improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ee7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建代理并运行\n",
    "agent = ActorCriticAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c36da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba838a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练结果\n",
    "agent.plot_training_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42af222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练评估\n",
    "agent.evaluate_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 测试\n",
    "test_rewards = agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试评估\n",
    "agent.evaluate_testing(test_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
