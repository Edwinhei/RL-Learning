{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f0f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 环境设置：5x5 网格世界\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = 5\n",
    "        self.start = (0, 0)  # 起点\n",
    "        self.goal = (4, 4)   # 目标\n",
    "        self.actions = ['up', 'down', 'left', 'right']  # 动作空间\n",
    "        self.action_to_index = {a: i for i, a in enumerate(self.actions)}\n",
    "        self.state = self.start  # 当前状态\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        # 根据动作更新位置\n",
    "        if action == 'up':\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 'down':\n",
    "            x = min(x + 1, self.grid_size - 1)\n",
    "        elif action == 'left':\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 'right':\n",
    "            y = min(y + 1, self.grid_size - 1)\n",
    "\n",
    "        next_state = (x, y)\n",
    "        # 计算奖励\n",
    "        if next_state == self.goal:\n",
    "            reward = 100  # 到达目标\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # 每移动一步 -1\n",
    "            if next_state == self.state:  # 撞墙\n",
    "                reward = -10\n",
    "            done = False\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823a0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA 算法\n",
    "class SARSAAgent:\n",
    "    def __init__(self, grid_size, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.actions = actions\n",
    "        self.action_to_index = {a: i for i, a in enumerate(actions)}\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # 探索率\n",
    "        # 初始化 Q 表：(状态, 动作) -> Q 值\n",
    "        self.q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:  # 探索\n",
    "            return random.choice(self.actions)\n",
    "        else:  # 利用\n",
    "            x, y = state\n",
    "            action_idx = np.argmax(self.q_table[x, y])\n",
    "            return self.actions[action_idx]\n",
    "        \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action):\n",
    "        x, y = state\n",
    "        nx, ny = next_state\n",
    "        action_idx = self.action_to_index[action]\n",
    "        next_action_idx = self.action_to_index[next_action]\n",
    "        # SARSA 更新公式\n",
    "        self.q_table[x, y, action_idx] += self.alpha * (\n",
    "            reward + self.gamma * self.q_table[nx, ny, next_action_idx] - self.q_table[x, y, action_idx]\n",
    "        )\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        x, y = state\n",
    "        action_idx = np.argmax(self.q_table[x, y])\n",
    "        return self.actions[action_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52525938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成！以下是从起点到目标的最优路径：\n",
      "(0, 0) -> (0, 1) -> (1, 1) -> (2, 1) -> (3, 1) -> (3, 2) -> (3, 3) -> (4, 3) -> (4, 4) -> 到达目标！\n"
     ]
    }
   ],
   "source": [
    "# 训练和测试\n",
    "def train():\n",
    "    env = GridWorld()\n",
    "    agent = SARSAAgent(env.grid_size, env.get_action_space())\n",
    "\n",
    "    # 训练参数\n",
    "    num_episodes = 1000\n",
    "    max_steps = 100\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        action = agent.choose_action(state)  # 初始动作\n",
    "        for step in range(max_steps):\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = agent.choose_action(next_state)  # SARSA 的在策略特性\n",
    "            agent.update(state, action, reward, next_state, next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # 测试：打印最优路径\n",
    "    print(\"训练完成！以下是从起点到目标的最优路径：\")\n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    steps = 0\n",
    "    while state != env.goal and steps < max_steps:\n",
    "        action = agent.get_policy(state)\n",
    "        state, _, done = env.step(action)\n",
    "        path.append(state)\n",
    "        steps += 1\n",
    "\n",
    "    for s in path:\n",
    "        print(s, end=\" -> \")\n",
    "    print(\"到达目标！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decacd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
