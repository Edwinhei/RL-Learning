{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c75dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练改进后的 Dueling DQN...\n",
      "Episode 0/1000, Reward: -204.12318096323662, Epsilon: 0.948\n",
      "Episode 100/1000, Reward: 266.309430861578, Epsilon: 0.010\n",
      "Episode 200/1000, Reward: 272.61855883034536, Epsilon: 0.010\n",
      "Episode 300/1000, Reward: 273.69067030753513, Epsilon: 0.010\n",
      "Episode 400/1000, Reward: 288.7087505637114, Epsilon: 0.010\n",
      "Episode 500/1000, Reward: 275.0519178867045, Epsilon: 0.010\n",
      "Episode 600/1000, Reward: 284.21615728420375, Epsilon: 0.010\n",
      "Episode 700/1000, Reward: -55.43972370006725, Epsilon: 0.010\n",
      "Episode 800/1000, Reward: 57.18698442436812, Epsilon: 0.010\n",
      "Episode 900/1000, Reward: 293.54411886000304, Epsilon: 0.010\n",
      "\n",
      "开始测试和可视化...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwin/miniconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/edwin/learn/RL/code/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1, Reward: 273.1900862975189\n",
      "Test Episode 2, Reward: 262.0296100464012\n",
      "Test Episode 3, Reward: 215.9819266426177\n",
      "Test Episode 4, Reward: 155.30221958491722\n",
      "Test Episode 5, Reward: 268.07588474504547\n",
      "Test Episode 6, Reward: 271.9226480280818\n",
      "Test Episode 7, Reward: 269.4135527644853\n",
      "Test Episode 8, Reward: 274.2472508760417\n",
      "Test Episode 9, Reward: 275.4015099810341\n",
      "Test Episode 10, Reward: 265.4954523342444\n",
      "\n",
      "模型评估：\n",
      "平均测试奖励：253.11 ± 36.56\n",
      "最后 100 回合平均训练奖励：233.91\n",
      "训练奖励曲线已保存为 'improved_dueling_dqn_training_rewards.png'\n",
      "测试视频已保存至 'videos' 文件夹\n",
      "表现一般。代理需要更多训练以提升性能。\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Dueling DQN 网络（无均值归一化）\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        # 共享特征层\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 状态价值流 (V(s))\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        # 动作优势流 (A(s, a))\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_layer(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        # 修改：直接使用 V(s) + A(s, a)，无均值归一化\n",
    "        q_values = value + advantage\n",
    "        return q_values\n",
    "\n",
    "# 优先经验回放缓冲区\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # 优先级系数\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        experience = self.experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max_priority)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # 计算重要性采样权重\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        state, action, reward, next_state, done = zip(*experiences)\n",
    "        return (np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done),\n",
    "                indices, np.array(weights, dtype=np.float32))\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority + 1e-5  # 避免优先级为 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Dueling DQN 代理（结合 Double DQN 和 PER）\n",
    "class DuelingDQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 主网络和目标网络\n",
    "        self.q_network = DuelingDQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = DuelingDQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0005)  # 降低学习率\n",
    "        \n",
    "        # 超参数\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999  # 更慢的衰减\n",
    "        self.batch_size = 64\n",
    "        self.memory = PrioritizedReplayBuffer(10000)\n",
    "        self.target_update_freq = 10  # 更频繁的更新\n",
    "        self.beta = 0.4  # 初始 beta，用于重要性采样\n",
    "        self.beta_increment = 0.001  # beta 逐渐增加\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def update(self, step):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 从优先经验回放缓冲区采样\n",
    "        state, action, reward, next_state, done, indices, weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = torch.LongTensor(action).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).to(self.device)\n",
    "\n",
    "        # 计算 Q 值（Double DQN）\n",
    "        q_values = self.q_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_actions = self.q_network(next_state).argmax(1, keepdim=True)\n",
    "        next_q_values = self.target_network(next_state).gather(1, next_actions).squeeze(1)\n",
    "        target = reward + (1 - done) * self.gamma * next_q_values\n",
    "\n",
    "        # 计算 TD 误差并更新优先级\n",
    "        td_errors = (q_values - target).abs().detach().cpu().numpy()\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        # 计算损失（考虑重要性采样权重）\n",
    "        loss = (nn.MSELoss(reduction='none')(q_values, target) * weights).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 更新探索率和 beta\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "\n",
    "        # 定期更新目标网络\n",
    "        if step % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def save_model(self, path=\"improved_dueling_dqn_cartpole.pth\"):\n",
    "        torch.save(self.q_network.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path=\"improved_dueling_dqn_cartpole.pth\"):\n",
    "        self.q_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# 训练函数\n",
    "def train_dueling_dqn():\n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    agent = DuelingDQNAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
    "    \n",
    "    num_episodes = 1000  # 增加训练回合\n",
    "    max_steps = 500\n",
    "    scores = []\n",
    "    step = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            agent.update(step)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes}, Reward: {episode_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    agent.save_model()\n",
    "    \n",
    "    # 绘制训练过程中的奖励曲线\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Training Rewards Over Episodes (Improved Dueling DQN)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"improved_dueling_dqn_training_rewards.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return agent, scores\n",
    "\n",
    "# 测试和可视化\n",
    "def test_dueling_dqn(agent):\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda x: True)\n",
    "    agent.epsilon = 0.0  # 测试时禁用探索\n",
    "\n",
    "    num_test_episodes = 10  # 增加测试回合\n",
    "    test_rewards = []\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < 500:\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            episode_reward += reward\n",
    "            t += 1\n",
    "\n",
    "        test_rewards.append(episode_reward)\n",
    "        print(f\"Test Episode {episode+1}, Reward: {episode_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    return test_rewards\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    # 训练\n",
    "    print(\"开始训练改进后的 Dueling DQN...\")\n",
    "    agent, scores = train_dueling_dqn()\n",
    "\n",
    "    # 测试和可视化\n",
    "    print(\"\\n开始测试和可视化...\")\n",
    "    test_rewards = test_dueling_dqn(agent)\n",
    "\n",
    "    # 评估表现\n",
    "    avg_reward = np.mean(test_rewards)\n",
    "    std_reward = np.std(test_rewards)\n",
    "    print(f\"\\n模型评估：\")\n",
    "    print(f\"平均测试奖励：{avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "    print(f\"最后 100 回合平均训练奖励：{np.mean(scores[-100:]):.2f}\")\n",
    "    print(\"训练奖励曲线已保存为 'improved_dueling_dqn_training_rewards.png'\")\n",
    "    print(\"测试视频已保存至 'videos' 文件夹\")\n",
    "    if avg_reward > 475:\n",
    "        print(\"表现优秀！代理成功学会了保持杆子平衡，接近最大奖励 500。\")\n",
    "    elif avg_reward > 300:\n",
    "        print(\"表现良好！代理能够保持平衡一段时间，但还有改进空间。\")\n",
    "    else:\n",
    "        print(\"表现一般。代理需要更多训练以提升性能。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92697df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"gymnasium[other]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f403b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
