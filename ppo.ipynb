{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85646722",
   "metadata": {},
   "source": [
    "# 这个版本错误挺多！\n",
    "\n",
    "有诸多问题..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11680ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actor_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 每批次训练轮数\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# actor_model是策略模型，即要训练的大模型\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# critic_model是状态价值模型，还是动作价值价值模型？这个模型是来自哪里？怎么设计这个模型的网络结构\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 优化器\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m actor_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(actor_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     17\u001b[0m critic_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(critic_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 工具函数\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actor_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "# 超参数\n",
    "gamma = 0.99  # 折扣因子\n",
    "lambda_ = 0.95  # GAE平滑参数\n",
    "epsilon = 0.2  # PPO剪切参数\n",
    "beta = 0.01  # KL惩罚系数\n",
    "value_loss_rate = 0.5  # 值损失权重\n",
    "epochs = 10  # 每批次训练轮数\n",
    "\n",
    "# actor_model是策略模型，即要训练的大模型\n",
    "# critic_model是状态价值模型，还是动作价值价值模型？这个模型是来自哪里？怎么设计这个模型的网络结构\n",
    "# 优化器\n",
    "actor_optimizer = optim.Adam(actor_model.parameters(), lr=1e-5)\n",
    "critic_optimizer = optim.Adam(critic_model.parameters(), lr=1e-5)\n",
    "\n",
    "# 工具函数\n",
    "def gather_log_probs(logits, actions):\n",
    "    \"\"\"从logits中提取实际动作的对数概率\"\"\"\n",
    "    return torch.log_softmax(logits, dim=-1).gather(dim=-1, index=actions.unsqueeze(-1))\n",
    "\n",
    "# 这个优势计算，\n",
    "def compute_advantages(values, rewards, gamma=0.99, lambda_=0.95):\n",
    "    \"\"\"计算GAE优势\"\"\"\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    for t in reversed(range(rewards.size(1))):\n",
    "        \"\"\"\n",
    "        (t < rewards.size(1)-1) 当达到序列最后一个元素是，输出false, 即0,未来奖励无 -> 这里反向操作，意味着，达到序列初始位置，前面无奖励\n",
    "        (t < rewards.size(1)-1) 当未达到序列最后一个元素时，输出为True, 即１,有未来奖励　-> 这里反向操作，意味着，未达到序列初始位置，前面有奖励\n",
    "        \n",
    "        疑问：这里每个状态下，动作的即使奖励是怎么计算得来的？\n",
    "        \"\"\"\n",
    "        delta = rewards[:, t] + gamma * values[:, t+1] * (t < rewards.size(1)-1) - values[:, t]\n",
    "        gae = delta + gamma * lambda_ * gae\n",
    "        advantages[:, t] = gae\n",
    "    return advantages\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "for batch_prompt in  prompt_dataset:\n",
    "    # 同一批次的提示词，要固定相同的长度对吗？最终模型输出也是相同长度，对吗？\n",
    "    batch_size, prompt_len = batch_prompt.size() # [batch_size, prompt_len]\n",
    "    \n",
    "    # 生成响应\n",
    "    # 输出生成序列的同时，输出token的概率\n",
    "    batch_response, old_log_probs = actor_model.generate(batch_prompt, return_log_probs=True)\n",
    "    # 生成的序列长度是否都一致，通过结束符判断，序列实际长度？\n",
    "    seq_len = batch_response.size(1) # 生成序列长度\n",
    "    \n",
    "    # 奖励模型评分\n",
    "    # 拼接:提示词+SEP_TOKEN_ID+生成的回答\n",
    "    batch_data = torch.cat([batch_prompt, torch.tensor([[SEP_TOKEN_ID]] * batch_size).to(batch_prompt.device), batch_response], dim=1)\n",
    "    # 获取每个回答的得分(累积得分)\n",
    "    batch_scores = reward_model(batch_data) # 形状: [batch_size]\n",
    "    batch_scores = torch.clamp(batch_scores, -10.0, 10.0) # 奖励裁剪\n",
    "    \n",
    "    # 分配奖励（序列级奖励，仅最后时间步非零）\n",
    "    rewards = torch.zeros(batch_size, seq_len).to(batch_scores.device)\n",
    "    # 这么来看，reward_model输出的是最后一步的奖励吗？而不是序列的累积奖励值？\n",
    "    rewards[:, -1] = batch_scores\n",
    "    \n",
    "    # Actor旧策略概率\n",
    "    # old_actor_logits = actor_model.forward(batch_prompt) # [batch_size, seq_len, vocab_size]\n",
    "    \"\"\" \n",
    "    很奇怪啊，这里前向传播，输出每个动作的对数概率和\n",
    "    batch_response, old_log_probs = actor_model.generate(batch_prompt, return_log_probs=True)\n",
    "    这里面输出有什么不一样？不都是同样的吗？输出的概率不t也是同样的吗？\n",
    "    除非你actor_model分为目标和主网络模型，不然我想不通，为啥还要多此一举呢？\n",
    "    \"\"\"\n",
    "    # old_log_probs = gather_log_probs(old_actor_logits, batch_response)\n",
    "    \n",
    "    # Critic值\n",
    "    \"\"\" \n",
    "    这个评价模型是输出每个token的状态值V(token)，还是表达V_k(batch_promp+token_0+token_1+...+token_k)\n",
    "    这是一个gpt架构的模型，还是bert的模型？感觉上是bert模型更合适。我们得看后面生成的什么，和已经生成内容，来评估这个状态的得分？\n",
    "    \"\"\"\n",
    "    critic_values = critic_model.forward(batch_prompt, batch_response) # [batch_size,seq_len]\n",
    "    \n",
    "    # 参考模型概率\n",
    "    ref_logits = ref_model.forward(batch_prompt)  # [batch_size, seq_len, vocab_size]\n",
    "    \"\"\"\n",
    "    参考模型：实际上是actor_target_model对不对？稳定训练过程的？\n",
    "    \n",
    "    通过参考模型输出原先选择动作的对数概率，来比较KL散度，尽量让ＫＬ散度不要相差太大！\n",
    "    ref_logits = ref_model.forward(batch_prompt)  # [batch_size, seq_len, vocab_size]\n",
    "    \"\"\"\n",
    "    ref_log_probs = gather_log_probs(ref_logits, batch_response) # [batch_size, seq_len]\n",
    "\n",
    "    # KL散度公式正确吗？怎么感觉不对呢？不是前面还应该乘上old_probs吗？\n",
    "    # KL散度\n",
    "    kls = (old_log_probs - ref_log_probs).mean(dim=1)  # [batch_size]\n",
    "    \n",
    "    # GAE优势\n",
    "    \"\"\" \n",
    "    原来，只有最后一步有奖励值。前面步数的奖励值都为0\n",
    "    靠状态价值模型预测输出每个动作选择的优势？\n",
    "    \"\"\"\n",
    "    advantages = compute_advantages(critic_values, rewards, gamma, lambda_)\n",
    "    \n",
    "    returns = advantages + critic_values\n",
    "    \n",
    "    # PPO训练\n",
    "    for _ in range(epochs):\n",
    "        # 新策略概率\n",
    "        \"\"\" \n",
    "        这里由于模型已经在做梯度更新了，所以给出的策略是一直在变的。那么代表着输出token的新概率，即新策略\n",
    "        \"\"\"\n",
    "        new_actor_logits = actor.model.forward(batch_prompt)\n",
    "        new_log_probs = gather_log_probs(new_actor_logits, batch_response)\n",
    "        \n",
    "        # 新值\n",
    "        new_critic_values = critic_model.forward(batch_prompt, batch_response)\n",
    "        \n",
    "        # 概率比\n",
    "        \"\"\" \n",
    "        这是类似重要性采样，将原先概率分布采样得到的token序列映射到现有概率分布中来\n",
    "        \"\"\"\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs) # [batch_size, seq_len]\n",
    "        \n",
    "        \"\"\" \n",
    "        我的疑问：\n",
    "        这里的old_log_probs是不是with torch.no_grad()的？\n",
    "        advantages　是不是也是with torch.no_grad()的？\n",
    "        在计算损失时，梯度更新的只有new_log_probs\n",
    "        \"\"\"\n",
    "        # PPO损失\n",
    "        surr1 = ratio * advantages\n",
    "        # 优势裁剪，防止策略过度更新\n",
    "        surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        loss_ppo = -torch.mean(torch.min(surr1, surr2))\n",
    "        \n",
    "        \"\"\" \n",
    "        我的疑问：\n",
    "        这里的returns是不是with torch.no_grad()的？\n",
    "        在计算损失时，梯度更新的只有new_critic_values\n",
    "        \"\"\"\n",
    "        # 值损失\n",
    "        loss_state_value = torch.mean((returns - new_critic_values)**2)\n",
    "        \n",
    "        \n",
    "        \"\"\" \n",
    "        beta*kls.mean() 这一项，是kls = (old_log_probs - ref_log_probs).mean(dim=1)  # [batch_size]\n",
    "        他不会进行梯度下降吧？ 他是老策略与参考模型的kls计算，不会进行梯度下降吧？只是一个正则项，对吗？\n",
    "        真正进行梯度计算的是不是只有loss_ppo中的new_log_probs和value_loss_rate中的new_critic_values？\n",
    "        \"\"\"\n",
    "        loss = loss_ppo + value_loss_rate*loss_state_value + beta*kls.mean()\n",
    "        \n",
    "        # 优化\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cfa81",
   "metadata": {},
   "source": [
    "# 修正版\n",
    "\n",
    "有诸多问题..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ce1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "# 超参数\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "epsilon = 0.2\n",
    "beta = 0.01\n",
    "value_loss_rate = 0.5\n",
    "epochs = 10\n",
    "SEP_TOKEN_ID = 102  # 假设的分隔符Token ID\n",
    "PAD_TOKEN_ID = 0  # 填充Token ID\n",
    "\n",
    "# 一般是bert或gpt模型，这里只是示例，一般会和actor模型共享几个层\n",
    "# Critic模型示例\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, prompts, responses):\n",
    "        inputs = torch.cat([prompts, responses], dim=1)\n",
    "        outputs = self.bert(inputs).last_hidden_state\n",
    "        values = self.value_head(outputs).squeeze(-1)  # [batch_size, seq_len]\n",
    "        return values\n",
    "    \n",
    "# 优化器\n",
    "actor_optimizer = optim.Adam(actor_model.parameters(), lr=1e-5)\n",
    "critic_optimizer = optim.Adam(critic_model.parameters(), lr=1e-5)\n",
    "\n",
    "# 工具函数\n",
    "def gather_log_probs(logits, actions):\n",
    "    return torch.log_softmax(logits, dim=-1).gather(dim=-1, index=actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def compute_advantages(values, rewards, gamma=0.99, lambda_=0.95):\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    for t in reversed(range(rewards.size(1))):\n",
    "        delta = rewards[:, t] + gamma * values[:, t+1] * (t < rewards.size(1)-1) - values[:, t]\n",
    "        gae = delta + gamma * lambda_ * gae\n",
    "        advantages[:, t] = gae\n",
    "    return advantages\n",
    "\n",
    "# 训练循环\n",
    "for batch_prompt in prompt_dataset:\n",
    "    batch_size, prompt_len = batch_prompt.size()  # [batch_size, prompt_len]\n",
    "    prompt_mask = (batch_prompt != PAD_TOKEN_ID).long()  # 注意力掩码\n",
    "    \n",
    "    # 生成响应\n",
    "    with torch.no_grad():\n",
    "        batch_response, old_log_probs = actor_model.generate(batch_prompt, max_length=100, return_log_probs=True)\n",
    "    seq_len = batch_response.size(1)\n",
    "    response_mask = (batch_response != PAD_TOKEN_ID).long()\n",
    "    \n",
    "    # 奖励模型评分\n",
    "    batch_data = torch.cat([batch_prompt, torch.tensor([[SEP_TOKEN_ID]] * batch_size).to(batch_prompt.device), batch_response], dim=1)\n",
    "    batch_scores = reward_model(batch_data)  # [batch_size]\n",
    "    batch_scores = torch.clamp(batch_scores, -10.0, 10.0)\n",
    "    \n",
    "    # 分配奖励\n",
    "    rewards = torch.zeros(batch_size, seq_len).to(batch_scores.device)\n",
    "    rewards[:, -1] = batch_scores\n",
    "    \n",
    "    # Critic值\n",
    "    critic_values = critic_model(batch_prompt, batch_response)  # [batch_size, seq_len]\n",
    "    \n",
    "    # GAE优势和回报\n",
    "    advantages = compute_advantages(critic_values.detach(), rewards, gamma, lambda_)\n",
    "    returns = (advantages + critic_values).detach()\n",
    "    \n",
    "    # 参考模型概率\n",
    "    ref_logits = ref_model.forward(batch_prompt)  # [batch_size, seq_len, vocab_size]\n",
    "    ref_log_probs = gather_log_probs(ref_logits, batch_response)\n",
    "    \n",
    "    # PPO训练\n",
    "    for _ in range(epochs):\n",
    "        # 新策略概率\n",
    "        new_actor_logits = actor_model.forward(batch_prompt)\n",
    "        new_log_probs = gather_log_probs(new_actor_logits, batch_response)\n",
    "        \n",
    "        # 新值\n",
    "        new_critic_values = critic_model.forward(batch_prompt, batch_response)\n",
    "        \n",
    "        # 概率比\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "        \n",
    "        # PPO损失\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages\n",
    "        loss_ppo = -torch.mean(torch.min(surr1, surr2) * response_mask)\n",
    "        \n",
    "        # 值损失\n",
    "        loss_state_value = torch.mean(((returns - new_critic_values)**2) * response_mask)\n",
    "        \n",
    "        # KL散度（基于新策略）\n",
    "        new_probs = torch.exp(new_log_probs)\n",
    "        ref_probs = torch.exp(ref_log_probs)\n",
    "        kls = new_probs * (new_log_probs - ref_log_probs)\n",
    "        kls = kls.sum(dim=1) * response_mask[:, -1]  # 考虑序列级KL\n",
    "        \n",
    "        # 总损失\n",
    "        loss = loss_ppo + value_less_rate * loss_state_value + beta * kls.mean()\n",
    "        \n",
    "        # 优化\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
